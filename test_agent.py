# =====================================
# âœ… test_agent.py â€” Bosch ä»£ç† + Azure æ¨¡å‹ç‰ˆæœ¬æ£€æµ‹ + LangGraph æµ‹è¯•ï¼ˆæ™ºèƒ½åˆ†æµç‰ˆï¼‰
# =====================================
import sys, os, warnings, httpx, json, time, traceback
from dotenv import load_dotenv
from perf_aop import aop_inject_timing

start_all = time.time()
warnings.filterwarnings("ignore")

# 1ï¸âƒ£ è·¯å¾„è®¾ç½®
sys.path.append(os.path.dirname(__file__))
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

# 2ï¸âƒ£ è‡ªåŠ¨åŠ è½½ .env
env_path = os.path.join(os.path.dirname(__file__), ".env")
load_dotenv(dotenv_path=env_path)
print(f"âœ… å·²åŠ è½½ .env æ–‡ä»¶: {env_path}")

# 3ï¸âƒ£ ç¯å¢ƒå˜é‡ä¸ä»£ç†è®¾ç½®
proxy = os.getenv("PROXY_URL", "").strip()
tavily_key = os.getenv("TAVILY_API_KEY", "").strip()
azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "").strip()
azure_api_key = os.getenv("AZURE_OPENAI_API_KEY", "").strip()

if tavily_key:
    print(f"âœ… æ£€æµ‹åˆ° Tavily API Key: {tavily_key[:15]}... (å·²éšè—ååŠéƒ¨åˆ†)")
else:
    print("âš ï¸ æœªæ£€æµ‹åˆ° TAVILY_API_KEY")

# ğŸ”§ Bosch ä»£ç†å¼ºåˆ¶å¯ç”¨
proxy = proxy or "http://fun4wx:qawaearata0A%21@rb-proxy-unix-szh.bosch.com:8080"
os.environ["HTTP_PROXY"] = proxy
os.environ["HTTPS_PROXY"] = proxy
os.environ["ALL_PROXY"] = proxy
os.environ["NO_PROXY"] = "localhost,127.0.0.1"
print("âœ… å¼ºåˆ¶å…¨å±€ä»£ç†å¯ç”¨:", proxy)

# 4ï¸âƒ£ æµ‹è¯• Azure Endpoint è¿é€šæ€§
can_access_azure = False
if azure_endpoint and azure_api_key:
    try:
        print(f"ğŸ” æµ‹è¯• Azure Endpoint: {azure_endpoint}")
        base_url = azure_endpoint.split("/openai/")[0]
        with httpx.Client(proxy=proxy, verify=False, trust_env=True, timeout=10.0) as client:
            resp = client.get(base_url)
            print("ğŸ”— Azure HTTP æµ‹è¯•çŠ¶æ€ç :", resp.status_code)
            if resp.status_code < 500:
                can_access_azure = True
    except Exception as e:
        print("âš ï¸ Azure æµ‹è¯•å¤±è´¥:", e)
else:
    print("âš ï¸ AZURE_OPENAI_ENDPOINT æˆ– AZURE_OPENAI_API_KEY æœªé…ç½®")
print("ğŸŒ Azure å¯è®¿é—®ï¼Ÿ", can_access_azure)

# âœ… 4.5ï¸âƒ£ æ£€æŸ¥éƒ¨ç½²æ¨¡å‹
if can_access_azure:
    try:
        print("\nğŸ” æŸ¥è¯¢éƒ¨ç½² 'gpt-5' å®é™…æ¨¡å‹...\n")
        deployment_name = "gpt-5"
        api_url = f"{azure_endpoint}openai/deployments/{deployment_name}/chat/completions?api-version=2025-01-01-preview"
        payload = {"messages": [{"role": "user", "content": "Say hello"}], "temperature": 1.0}
        headers = {"api-key": azure_api_key, "Content-Type": "application/json"}
        with httpx.Client(proxy=proxy, verify=False, trust_env=True, timeout=20.0) as client:
            r = client.post(api_url, headers=headers, json=payload)
            if r.status_code == 200:
                print(f"ğŸ“¦ æ¨¡å‹ç‰ˆæœ¬: {r.json().get('model', '(unknown)')}")
            else:
                print(f"âš ï¸ æŸ¥è¯¢å¤±è´¥ï¼ŒHTTP {r.status_code}")
    except Exception as e:
        print("âš ï¸ æŸ¥è¯¢æ¨¡å‹åç§°å¤±è´¥:", e)

# âœ… 5ï¸âƒ£ æµ‹è¯• GPT-5 è°ƒç”¨
if can_access_azure:
    try:
        print("\nğŸ§  æµ‹è¯• Azure GPT-5 æ¨¡å‹è°ƒç”¨...\n")
        from langchain_openai import AzureChatOpenAI
        llm = AzureChatOpenAI(
            deployment_name="gpt-5",
            api_key=azure_api_key,
            azure_endpoint=azure_endpoint,
            api_version="2025-01-01-preview",
            temperature=1.0,
        )
        start_time = time.time()
        resp = llm.invoke("Reply with only: OK")
        print(f"âœ… æ¨¡å‹å›å¤: {resp.content.strip()}")
        print(f"â±ï¸ è°ƒç”¨è€—æ—¶: {time.time() - start_time:.2f} ç§’")
    except Exception as e:
        print("âŒ GPT-5 æ¨¡å‹è°ƒç”¨å¤±è´¥:", e)
else:
    print("âš ï¸ è·³è¿‡ GPT-5 è°ƒç”¨æµ‹è¯•ï¼ˆAzure ä¸å¯è®¿é—®ï¼‰")

# ===========================================================
# âœ… æ™ºèƒ½è¯†åˆ«é€»è¾‘ï¼šåˆé‡‘ â†’ LangGraphï¼›éåˆé‡‘ â†’ GenericPriceFinderTool
# ===========================================================
from agent import chain, save_json_report, parse_user_inputs_from_query
from tools.generic_price_finder_tool import GenericPriceFinderTool
from langchain_core.messages import HumanMessage
from langchain_openai import AzureChatOpenAI

def is_alloy_query(user_query: str) -> bool:
    """ç”¨ GPT åˆ¤æ–­æ˜¯å¦ä¸ºåˆé‡‘ç±»ææ–™"""
    try:
        llm = AzureChatOpenAI(
            deployment_name="gpt-5",
            api_key=os.getenv("AZURE_OPENAI_API_KEY", ""),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT", ""),
            api_version="2025-01-01-preview",
            temperature=1.0,
        )
        prompt = f"""
        åˆ¤æ–­ä»¥ä¸‹è¾“å…¥æ˜¯å¦æ¶‰åŠé‡‘å±åˆé‡‘ï¼ˆä¾‹å¦‚ AlSi9Mnã€CuAl10Ni2ã€AA6061ã€FeCrNi ç­‰ï¼‰ã€‚
        å¦‚æœæ˜¯åˆé‡‘ï¼Œè¯·åªè¾“å‡º â€œYESâ€ï¼›å¦åˆ™è¾“å‡º â€œNOâ€ã€‚
        è¾“å…¥ï¼š{user_query}
        """
        resp = llm.invoke(prompt)
        answer = resp.content.strip().upper()
        print(f"ğŸ§© GPT åˆ¤æ–­ç»“æœï¼š{answer}")
        return "YES" in answer
    except Exception as e:
        print("âš ï¸ GPT åˆ¤æ–­å¤±è´¥ï¼Œé»˜è®¤è®¤ä¸ºæ˜¯åˆé‡‘ã€‚é”™è¯¯ï¼š", e)
        return True

def run_smart_query(user_query: str):
    """æ ¹æ® query è‡ªåŠ¨åˆ¤æ–­æ‰§è¡Œè·¯å¾„"""
    print("\n==================== æ™ºèƒ½è¯†åˆ«æ‰§è¡Œ ====================\n")
    print(f"ğŸ§  ç”¨æˆ·è¾“å…¥: {user_query}\n")

    if is_alloy_query(user_query):
        print("âœ… åˆ¤æ–­ä¸ºåˆé‡‘ â†’ æ‰§è¡Œ LangGraph ä¸»é“¾")
        start = time.time()
        result = chain.invoke(
            {"messages": [HumanMessage(content=user_query)]},
            config={"configurable": {"thread_id": "smart-auto"}}
        )
        print(f"â±ï¸ LangGraph æ‰§è¡Œè€—æ—¶: {time.time() - start:.2f} ç§’\n")

        # è¾“å‡ºç»“æœä¸ä¿å­˜
        for m in result.get("messages", []):
            print(f"[{m.__class__.__name__}] å†…å®¹é¢„è§ˆ:\n{m.content[:400]}...\n")
        save_json_report({
            "messages": result.get("messages", []),
            "user_inputs": parse_user_inputs_from_query(user_query)
        })
        print("âœ… å·²ä¿å­˜åˆ°æœ¬åœ°/äº‘ç«¯ã€‚")
        return result
    else:
        print("âœ… åˆ¤æ–­ä¸ºéåˆé‡‘ â†’ æ‰§è¡Œ GenericPriceFinderTool")
        tool = GenericPriceFinderTool()
        start = time.time()
        output = tool.run(user_query)
        print(f"â±ï¸ GenericPriceFinderTool æ‰§è¡Œè€—æ—¶: {time.time() - start:.2f} ç§’\n")
        print("è¾“å‡ºç»“æœé¢„è§ˆï¼ˆå‰ 800 å­—ç¬¦ï¼‰:\n", output[:800])
        return output

# ===========================================================
# âœ… å•ä¸€ query æµ‹è¯•å…¥å£ï¼ˆè‡ªåŠ¨åˆ¤æ–­æ‰§è¡Œï¼‰
# ===========================================================
# âš ï¸ åªæ”¹è¿™ä¸€è¡Œå³å¯ï¼šå¦‚æœæ¢æˆ Loctite 603 ä¼šè‡ªåŠ¨è¯†åˆ«ä¸ºéåˆé‡‘
query = "calculate this material AlSi9Mn price; Location=Ningbo, Zhejiang; supplier_code=97036203; part_number=044220003G; sub_process_step=AlSi9Mn; process_type=raw_material"
# query = "Find the price of Loctite 603"

print("\n==================== æ™ºèƒ½æµ‹è¯•å¼€å§‹ ====================\n")
try:
    run_smart_query(query)
except Exception as e:
    print(f"âŒ æ‰§è¡Œå‡ºé”™: {e}")
    traceback.print_exc()

print(f"\nğŸ•’ ç¨‹åºæ€»è€—æ—¶: {time.time() - start_all:.2f} ç§’")

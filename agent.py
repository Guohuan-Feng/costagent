# -*- coding: utf-8 -*-
"""
agent.py â€” LLM-powered process + price reasoning chain
Fully dynamic (no hardcoded alloy composition).
Supports process_finder_tool + price_finder_tool.
"""

import warnings
warnings.filterwarnings("ignore")
from langchain_openai import AzureChatOpenAI
import os, re, json, time, itertools, csv
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, wait
from typing import Any, Dict, Iterable, List, Sequence, Optional, Union
from typing_extensions import TypedDict
from pydantic import BaseModel, Field, AliasChoices, ConfigDict
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import (
    BaseMessage, FunctionMessage, HumanMessage,
    SystemMessage, AIMessage
)
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableBranch, chain as as_runnable
from langchain_core.tools import BaseTool
from output_parser import LLMCompilerPlanParser, Task
from prompt_template import SYSTEM_PROMPT, JOINER_PROMPT
from tools.process_finder_tool import ProcessFinderTool
from tools.price_finder_tool import PriceFinderTool
from datetime import datetime, timezone, timedelta

# ===========================================================
# === Azure OpenAI æ¨¡å‹åˆå§‹åŒ– ===
# ===========================================================
load_dotenv()
llm = AzureChatOpenAI(
    deployment_name="gpt-5",
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_version="2025-01-01-preview",
    temperature=1.0
)

# ===========================================================
# === æ³¨å†Œå·¥å…· ===
# ===========================================================
process_tool = ProcessFinderTool(llm).as_tool()
price_tool = PriceFinderTool(llm).as_tool()
tools = [process_tool, price_tool]

# ===========================================================
# === ä»£å·æå–å™¨ï¼ˆç¨³å®šå¯å¤ç”¨ï¼‰ ===
# ===========================================================
def _extract_alloy_code(text: str) -> str:
    s = (text or "").strip()
    for m in re.finditer(r"\(([^)]+)\)", s):
        inner = m.group(1)
        code = _extract_alloy_code(inner)
        if code:
            return code

    patterns = [
        r"\bEN\s?AC[-\s]?\d{5}\b",
        r"\bADC\d{2}\b",
        r"\b[1-7]\d{3}\b",
        r"\bA3\d{2}\b",
    ]
    for p in patterns:
        m = re.search(p, s, re.I)
        if m: return m.group(0)

    elem = r"(?:[A-Z][a-z]?)(?:\d+(?:\.\d+)?)?"
    m = re.search(rf"\b{elem}(?:[-]{elem}|{elem})+\b", s)
    if m: return m.group(0)
    m = re.search(rf"\b{elem}[-]{elem}\b", s)
    if m: return m.group(0)
    return ""

# ===========================================================
# === æ–°å¢ï¼šæ™ºèƒ½åˆ†æµå…¥å£ï¼ˆä¸ä¿®æ”¹ä»»ä½•åŸæœ‰é€»è¾‘ï¼‰ ===
# ===========================================================

def _llm_is_alloy_name(text: str) -> bool:
    """
    ç”¨ GPT-5 æ™ºèƒ½åˆ¤æ–­æ˜¯å¦ä¸ºé‡‘å±åˆé‡‘åï¼ˆæ— æ­£åˆ™ã€æ— ç¡¬ç¼–ç ï¼‰ã€‚
    ä»…è¿”å› True / Falseã€‚
    """
    s = (text or "").strip()
    if not s:
        return False
    try:
        check_llm = AzureChatOpenAI(
            deployment_name="gpt-5",
            api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            api_version="2025-01-01-preview",
            temperature=1.0
        )
        prompt = f"""
You are a materials expert.
Determine whether the following input refers to a **metal alloy** name or not.

Rules:
- Return ONLY "True" or "False".
- Examples of alloys: AlSi9Mn, CuAl10Ni2, ADC12, EN AC-46000, 6061, Brass, Bronze.
- Examples of non-alloys: Loctite 603, Epoxy 1200, Superglue, Silicone Sealant.
- Be concise. No explanation.

Input: {s}
"""
        resp = check_llm.invoke(prompt)
        ans = (getattr(resp, "content", str(resp)) or "").strip().lower()
        return ans.startswith("t")
    except Exception as e:
        print(f"[WARN] Alloy LLM check failed: {e}")
        return False


def smart_run(user_query: str):
    """
    æ™ºèƒ½åˆ¤æ–­ç”¨æˆ·è¾“å…¥ç±»å‹ï¼Œå¹¶é€‰æ‹©å¯¹åº”å·¥å…·é“¾è¿è¡Œï¼š
      - å¦‚æœ LLM åˆ¤æ–­æ˜¯åˆé‡‘ï¼Œåˆ™ä¿æŒåŸæµç¨‹ï¼ˆprocess + priceï¼‰
      - å¦‚æœä¸æ˜¯åˆé‡‘ï¼ˆå¦‚ Loctite 603ï¼‰ï¼Œä»…è°ƒç”¨ generic_price_finder_tool
    """
    from tools.generic_price_finder_tool import GenericPriceFinderTool
    from tools.price_finder_tool import PriceFinderTool
    from tools.process_finder_tool import ProcessFinderTool
    from langchain_core.messages import HumanMessage

    print(f"\n[SMART] ğŸ§  åˆ¤æ–­ææ–™ç±»å‹ä¸­ï¼š{user_query}")
    is_alloy = _llm_is_alloy_name(user_query)
    if is_alloy:
        print("[SMART] âœ… GPT åˆ¤æ–­ï¼šè¿™æ˜¯åˆé‡‘ â†’ èµ°åŸå§‹é“¾è·¯ï¼ˆprocess + priceï¼‰")
        from agent import chain
        result = chain.invoke(
            {"messages": [HumanMessage(content=user_query)]},
            config={"configurable": {"thread_id": "smart-run"}}
        )
        return result
    else:
        print("[SMART] âš™ï¸ GPT åˆ¤æ–­ï¼šéåˆé‡‘ â†’ ä½¿ç”¨ GenericPriceFinderTool å•ç‹¬æ‰§è¡Œ")
        llm = AzureChatOpenAI(
            deployment_name="gpt-5",
            api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            api_version="2025-01-01-preview",
            temperature=1.0
        )
        tool = GenericPriceFinderTool(llm)
        output = tool.run(user_query)  # âœ… è¿™ä¸€è¡Œå¿…é¡»åŠ ä¸Š
        try:
            output_json = json.loads(output)
        except Exception:
            output_json = {"raw_output": output}

        if "final_price_cny_per_g" in output_json:
            output_json["total_cost"] = output_json["final_price_cny_per_g"]
            output_json["unit"] = "CNY/g"

        print("[SMART] ğŸ§¾ Generic tool è¾“å‡ºï¼š", output_json)
        return output_json



# ===========================================================
# === LangChain Planner åˆå§‹åŒ– ===
# ===========================================================
from prompt_template import SYSTEM_PROMPT
planning_prompt = ChatPromptTemplate.from_template(SYSTEM_PROMPT)

def render_tool_for_planning(tool: BaseTool, idx: int) -> str:
    arg_names = list(tool.args.keys()) if hasattr(tool, "args") else []
    sig = ", ".join([f"{a}=..." for a in arg_names]) if arg_names else ""
    return f"{idx}. {tool.name}({sig})\n    - {tool.description.strip()}"

THOUGHT_RE = re.compile(r"(?im)^\s*Thought\s*:\s*(.+?)(?:\n|$)")
def extract_thought(text: str) -> str:
    m = THOUGHT_RE.search(text or "")
    return (m.group(1).strip() if m else "").strip()

def create_planner_components(llm: BaseChatModel, tools: Sequence[BaseTool], base_prompt: ChatPromptTemplate):
    tool_descriptions = "\n".join(render_tool_for_planning(t, i+1) for i, t in enumerate(tools))
    planner_prompt = base_prompt.partial(
        replan="", num_tools=len(tools)+1, tool_descriptions=tool_descriptions)
    replanner_prompt = base_prompt.partial(
        replan=' - Continue planning using previous results.',
        num_tools=len(tools)+1, tool_descriptions=tool_descriptions,
    )

    def should_replan(state: list): return isinstance(state[-1], SystemMessage)
    def wrap_messages(state: list): return {"messages": state}
    def wrap_and_get_last_index(state: list):
        next_task = 0
        for message in state[::-1]:
            if isinstance(message, FunctionMessage):
                next_task = message.additional_kwargs["idx"] + 1
                break
        state[-1].content += f" - Begin counting at : {next_task}"
        return {"messages": state}

    planner_raw = RunnableBranch(
        (should_replan, wrap_and_get_last_index | replanner_prompt),
        wrap_messages | planner_prompt,
    ) | llm
    planner_tasks = RunnableBranch(
        (should_replan, wrap_and_get_last_index | replanner_prompt),
        wrap_messages | planner_prompt,
    ) | llm | LLMCompilerPlanParser(tools=tools)
    return planner_raw, planner_tasks

planner_raw, planner = create_planner_components(llm, tools, planning_prompt)

# ===========================================================
# === æ ¸å¿ƒä»»åŠ¡æ‰§è¡Œé€»è¾‘ ===
# ===========================================================
def _json_maybe_load(x):
    if isinstance(x, str):
        s = x.strip()
        if (s.startswith("{") and s.endswith("}")) or (s.startswith("[") and s.endswith("]")):
            try:
                return json.loads(s)
            except Exception:
                return x
    return x

def _walk_path(obj, path: str):
    if not path: return obj
    cur = obj
    for tok in path.split("."):
        cur = _json_maybe_load(cur)
        if isinstance(cur, list) and tok.isdigit():
            cur = cur[int(tok)]
        elif isinstance(cur, dict) and tok in cur:
            cur = cur[tok]
        else:
            raise KeyError(f"path '{path}' not found")
    return cur

_REF_ANY = re.compile(r"\$(\d+)(?:\.([A-Za-z0-9_\.]+))?")
def _resolve_inline_refs_in_text(text: str, observations: dict) -> str:
    def _sub(m: re.Match):
        idx = int(m.group(1))
        path = m.group(2) or ""
        base = _json_maybe_load(observations[idx])
        val = _walk_path(base, path)
        return json.dumps(val, ensure_ascii=False)
    return _REF_ANY.sub(_sub, text)

def _resolve_arg(arg, observations):
    if isinstance(arg, str) and "$" in arg:
        try: return _resolve_inline_refs_in_text(arg, observations)
        except Exception: return arg
    if isinstance(arg, list): return [_resolve_arg(a, observations) for a in arg]
    if isinstance(arg, dict): return {k: _resolve_arg(v, observations) for k, v in arg.items()}
    return arg

def _get_observations(messages: List[BaseMessage]) -> Dict[int, Any]:
    results = {}
    for message in messages[::-1]:
        if isinstance(message, FunctionMessage):
            results[int(message.additional_kwargs["idx"])] = message.content
    return results

def _execute_task(task, observations, config):
    tool_to_use = task["tool"]
    args = task["args"]
    try:
        resolved_args = _resolve_arg(args, observations)
    except Exception as e:
        return f"ERROR(Failed to resolve args: {repr(e)})"

    if tool_to_use.name == "price_finder":
        ctx_json = observations.get(1)  # âœ… process_finder çš„è¾“å‡ºæ°¸è¿œæ˜¯ idx=1
        if ctx_json:
            try:
                ctx = json.loads(ctx_json)
                resolved_args["PROCESS_CONTEXT"] = ctx
                print(f"[DEBUG] ğŸ”— å·²ä¼ é€’ PROCESS_CONTEXT ç»™ price_finder")
            except:
                print("[WARN] PROCESS_CONTEXT JSON è§£æå¤±è´¥")

    # âœ… ä»…åœ¨æ£€æµ‹åˆ°åˆé‡‘ä»£å·æ—¶æ‰“å°æç¤ºï¼ˆå®é™…æ¨ç†ç”± process_finder å†…éƒ¨å®Œæˆï¼‰
    if tool_to_use.name == "process_finder":
        alloy_text = resolved_args.get("user_request", "")
        code = _extract_alloy_code(alloy_text)
        if code:
            print(f"[DEBUG] âœ… æ£€æµ‹åˆ°åˆé‡‘ä»£å· {code}ï¼Œå°†åœ¨ process_finder å†…éƒ¨è¿›è¡Œæˆåˆ†æ¨ç†ã€‚")
        else:
            print(f"[DEBUG] âš ï¸ æœªæ£€æµ‹åˆ°ä»»ä½•åˆé‡‘ä»£å·ï¼Œç›´æ¥è¿›è¡Œç”Ÿäº§å·¥è‰ºæ¨ç†ã€‚")

    try:
        start = time.time()
        result = tool_to_use.invoke(resolved_args, config)
        end = time.time()
        print(f"[TIME] {tool_to_use.name} æ‰§è¡Œè€—æ—¶: {end - start:.2f} ç§’")
        return result
    except Exception as e:
        return f"ERROR(Failed to call {tool_to_use.name}: {repr(e)})"

# ===========================================================
# === è°ƒåº¦æ‰§è¡Œ ===
# ===========================================================
@as_runnable
def schedule_task(task_inputs, config):
    task, observations = task_inputs["task"], task_inputs["observations"]
    try:
        # æ­£å¸¸æ‰§è¡Œä»»åŠ¡
        observation = _execute_task(task, observations, config)
    except Exception as e:
        # æ•è·å¼‚å¸¸å¹¶å®‰å…¨æ ¼å¼åŒ–å †æ ˆ
        import traceback
        observation = "".join(traceback.format_exception(type(e), e, e.__traceback__))
    # æ— è®ºæˆåŠŸæˆ–å¤±è´¥éƒ½è®°å½•ç»“æœ
    observations[task["idx"]] = observation

def schedule_pending_task(task, observations, retry_after=0.2):
    while True:
        deps = task["dependencies"]
        if deps and (any([dep not in observations for dep in deps])):
            time.sleep(retry_after); continue
        schedule_task.invoke({"task": task, "observations": observations})
        break

@as_runnable
def schedule_tasks(scheduler_input: Dict[str, Any]) -> List[BaseMessage]:
    print("[DEBUG] è¿›å…¥ schedule_tasks")
    tasks = scheduler_input["tasks"]
    messages = scheduler_input["messages"]
    observations = _get_observations(messages)
    tool_messages: List[BaseMessage] = []

    with ThreadPoolExecutor() as executor:
        futures = []
        for task in tasks:
            deps = task["dependencies"]
            name = task["tool"].name if not isinstance(task["tool"], str) else task["tool"]
            idx = task["idx"]
            def _submit(_task=task, _name=name, _idx=idx):
                tool_messages.append(AIMessage(content=f"â³ Start: [{_idx}] {_name}"))
                schedule_task.invoke(dict(task=_task, observations=observations))
                obs = observations[_idx]

                # --- START MODIFICATION: PRINT FUNCTION MESSAGE ---
                print(f"\n===== ğŸ“ FunctionMessage (idx={_idx}, tool={_name}) =====")
                # å°è¯•æ¼‚äº®æ‰“å° JSON å†…å®¹
                try:
                    pretty_content = json.dumps(json.loads(str(obs)), indent=2, ensure_ascii=False)
                    print(pretty_content)
                except:
                    print(str(obs))
                print("====================================================\n")
                # --- END MODIFICATION ---

                tool_messages.append(FunctionMessage(
                    name=_name, content=str(obs),
                    additional_kwargs={"idx": _idx, "args": task["args"]},
                    tool_call_id=_idx))
                tool_messages.append(AIMessage(content=f"âœ… Done: [{_idx}] {_name}"))
            if deps and any([dep not in observations for dep in deps]):
                futures.append(executor.submit(schedule_pending_task, task, observations))
            else: _submit()
        wait(futures)
    return tool_messages

# ===========================================================
# === LangGraph æ„å»º ===
# ===========================================================
from langgraph.graph import END, StateGraph, START
from langgraph.graph.message import add_messages
from langchain_core.messages import SystemMessage
from langgraph.checkpoint.memory import MemorySaver

@as_runnable
def plan_and_schedule(state):
    print("[DEBUG] è¿›å…¥ plan_and_schedule")
    start_all = time.time()

    messages = state["messages"]

    # âœ… 1. ä¿å­˜æœ€åˆçš„äººç±»è¾“å…¥ï¼ˆåªä¿å­˜ä¸€æ¬¡ï¼‰
    if "original_user_request" not in state:
        for m in messages:
            if isinstance(m, HumanMessage):
                state["original_user_request"] = m.content
                print(f"[DEBUG] âœ… è®°å½• original_user_request = {state['original_user_request']}")
                break

    # âœ… 2. åˆ¤æ–­æ˜¯å¦ Replan è§¦å‘
    last_msg = messages[-1]
    is_replan = False
    try:
        repl = json.loads(last_msg.content)
        if repl.get("action") == "replan":
            is_replan = True
            print("[DEBUG] ğŸ” æ£€æµ‹åˆ° JOINER è§¦å‘ Replanï¼Œä½¿ç”¨ original_user_request è€Œä¸æ˜¯ feedback")
    except:
        pass

    # âœ… 3. å†³å®šè¿™è½®å·¥å…·è¾“å…¥çš„ user_request åº”è¯¥æ˜¯ä»€ä¹ˆï¼š
    if is_replan:
        user_request_for_tools = state["original_user_request"]
    else:
        # å¦‚æœä¸æ˜¯ Replanï¼Œæ­£å¸¸ä½¿ç”¨ messages[-1] æˆ– HumanMessage å†…å®¹
        # è¿™é‡Œä¿æŒä½ çš„åŸé€»è¾‘ï¼Œä¹Ÿå¯ä»¥ç»Ÿä¸€ç”¨ state["original_user_request"]
        user_request_for_tools = messages[-1].content

    # âœ… 4. ä½ çš„ç”¨æˆ·è¾“å…¥è§£æé€»è¾‘
    user_inputs = parse_user_inputs_from_query(state["original_user_request"])
    print(f"[DEBUG] âœ… è§£æçš„ç”¨æˆ·è¾“å…¥å­—æ®µ: {user_inputs}")
    state["user_inputs"] = user_inputs

    observations = _get_observations(messages)
    alloy_code = state.get("alloy_code")

    if not alloy_code:
        for m in messages:
            if isinstance(m, HumanMessage):
                code = _extract_alloy_code(str(m.content))
                if code:
                    alloy_code = code
                    print(f"[DEBUG] âœ… æå–åˆ°åˆé‡‘ä»£å· {alloy_code}")
                    break

    has_context = (1 in observations)
    task_list = []

    # âœ… åªæ”¹è¿™ä¸€å—çš„ argsï¼Œç¡®ä¿ä¸ä¼šæŠŠ feedback JSON ä¼ ç»™å·¥å…·
    if not has_context:
        task_list.append({
            "idx": 1,
            "tool": process_tool,
            "args": {"user_request": user_request_for_tools},
            "dependencies": []
        })
        print("[DEBUG] ğŸ“Œ éœ€è¦å·¥è‰º â†’ æ·»åŠ  process_finder")

    task_list.append({
        "idx": 2,
        "tool": price_tool,
        "args": {"user_request": user_request_for_tools},
        "dependencies": [1] if not has_context else []
    })
    print("[DEBUG] ğŸ“Œ æ·»åŠ  price_finder")

    # âœ… ä¿ç•™ä½ çš„ planner æ€è€ƒè®°å½•é€»è¾‘
    raw = planner_raw.invoke(messages)
    raw_content = getattr(raw, "content", "")
    
    # --- START MODIFICATION: PRINT PLANNER AIMESSAGE ---
    print("\n===== ğŸ§  Planner AIMessage (Thought/Action) =====")
    print(raw_content)
    print("================================================\n")
    # --- END MODIFICATION ---

    thought_text = extract_thought(raw_content)
    if thought_text:
        messages = messages + [AIMessage(content=f"Thought: {thought_text}")]

    print("[DEBUG] âœ… åŠ¨æ€ä»»åŠ¡åˆ—è¡¨: ", task_list)

    start_sched = time.time()
    tool_messages = schedule_tasks.invoke({"messages": messages, "tasks": task_list})
    end_sched = time.time()
    print(f"[TIME] schedule_tasks.invoke è€—æ—¶: {end_sched - start_sched:.2f} ç§’")

    total = time.time() - start_all
    print(f"[TIME] plan_and_schedule æ€»è€—æ—¶: {total:.2f} ç§’")

    return {"messages": messages + tool_messages, "alloy_code": alloy_code}



class FinalResponse(BaseModel):
    model_config = ConfigDict(populate_by_name=True)
    json_output: Optional[Dict[str, Any]] = Field(None) # <--- ä¿®æ”¹ä¸º Optional[Dict] = Field(None)

class Replan(BaseModel):
    model_config = ConfigDict(populate_by_name=True)
    feedback: Optional[str] = Field(None, validation_alias=AliasChoices("feedback", "Feedback")) # <--- ä¿®æ”¹ä¸º Optional[str] = Field(None)

class JoinOutputs(BaseModel):
    model_config = ConfigDict(populate_by_name=True)
    thought: str = Field(validation_alias=AliasChoices("thought", "Thought"))
    action: Union[FinalResponse, Replan]

joiner_prompt = ChatPromptTemplate.from_template(JOINER_PROMPT).partial(examples="")
joiner_llm = AzureChatOpenAI(
    deployment_name="gpt-5",
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_version="2025-01-01-preview",
    temperature=1.0
)
runnable = joiner_prompt | joiner_llm.with_structured_output(JoinOutputs, method="function_calling")
JOIN_SENTINEL = "__JOIN_DONE__"

def make_final_ai_md(text: str) -> str:
    return f"{text}\n\n"

def build_join_parser_with_context():
    def _parse_joiner_output(decision):
        print("[DEBUG] joiner è¾“å‡º decision:", decision)

        action = decision.action

        # âœ… æ­£ç¡®è§£æ json_output å¹¶ç¡®è®¤åŒ…å« total_cost
        if hasattr(action, "json_output"):
            result = action.json_output
            if isinstance(result, dict) and "total_cost" in result:
                
                # --- START MODIFICATION: PRINT JOINER AIMESSAGE ---
                final_json_content = json.dumps(result, indent=2, ensure_ascii=False)
                print("\n===== ğŸ’° Joiner AIMessage (Final Cost JSON) =====")
                print(final_json_content)
                print("==================================================\n")
                # --- END MODIFICATION ---
                
                msgs = [
                    AIMessage(content=final_json_content),
                    SystemMessage(content=JOIN_SENTINEL),
                ]
                return {"messages": msgs}

        # âŒ å¦‚æœä¸æ˜¯ json_output æˆ–ç¼ºå¤± total_cost â†’ Replan
        feedback = getattr(action, "feedback", "JOINER missing json_output.total_cost")
        msgs = [
            SystemMessage(content=json.dumps(
                {"action": "replan", "feedback": feedback},
                ensure_ascii=False
            ))
        ]
        return {"messages": msgs}

    return _parse_joiner_output




def select_recent_messages(state) -> dict:
    messages = state["messages"]; selected = []
    for msg in messages[::-1]:
        selected.append(msg)
        if isinstance(msg, HumanMessage): break
    return {"messages": selected[::-1]}

joiner = select_recent_messages | runnable | build_join_parser_with_context()
class State(TypedDict): 
    messages: List[BaseMessage]
    alloy_code: Optional[str]
checkpointer = None
graph_builder = StateGraph(State)
graph_builder.add_node("plan_and_schedule", plan_and_schedule)
graph_builder.add_node("join", joiner)
graph_builder.add_edge(START, "plan_and_schedule")
graph_builder.add_edge("plan_and_schedule", "join")

def should_continue(state):
    for m in state["messages"]:
        if isinstance(m, SystemMessage) and m.content == JOIN_SENTINEL:
            return END
    return "plan_and_schedule"

graph_builder.add_conditional_edges("join", should_continue)
chain = graph_builder.compile(checkpointer=checkpointer)
print("ğŸŸ¢ Chain execution finished.")

import os
import re
from langchain_core.messages import AIMessage


# --- agent.py ä¿®æ­£ç‰ˆï¼šæ”¯æŒ triple-quoted long_output è§£æ ---
import os, re, json
from datetime import datetime

def _extract_alloy_code_from_messages(messages) -> str:
    try:
        for msg in messages[::-1]:
            text = str(getattr(msg, "content", "") or "")
            code = _extract_alloy_code(text)
            if code:
                return code
        return "Alloy"
    except:
        return "Alloy"

def extract_long_output_from_text(text: str) -> str:

    # å…¼å®¹ llm ç”Ÿæˆçš„ä¸‰å¼•å·æ ¼å¼ï¼š
    # long_output=""" ... """
    # å¹¶å‰¥ç¦»å¤–å±‚ä»£ç å— ```...```

    # å…ˆæ‰¾ long_output=""" 
    pat = re.compile(r'long_output\s*=\s*"""\n?(.*?)"""', re.DOTALL)
    m = pat.search(text)
    if not m:
        return None
    content = m.group(1).strip()

    # å»æ‰ ``` åŒ…è£¹
    if content.startswith("```"):
        content = content.lstrip("`")
        # å»æ‰æœ«å°¾å¯¹åº”çš„ ```
        content = content.rstrip("`")
        # é˜²æ­¢è¯­è¨€æ ‡è®°å¦‚ ```markdown
        content = re.sub(r'^.*?\n', '', content, count=1)

    return content.strip()

import uuid, os, json
from datetime import datetime

def save_json_report(state):
    """
    è‡ªåŠ¨ä¿å­˜é•¿/çŸ­ JSONï¼Œå¹¶ä¸Šä¼ åˆ° Azure Blob Storage (rates/longjson, rates/shortjson)
    """
    messages = state.get("messages", [])
    # âœ… å°è¯•è¯»å– state ä¸­çš„ user_inputs
    user_inputs = state.get("user_inputs", {})

    # âœ… å¦‚æœä¸ºç©ºï¼Œåˆ™è‡ªåŠ¨ä»ç”¨æˆ· query é‡æ–°è§£æ
    if not user_inputs:
        for m in messages:
            if isinstance(m, HumanMessage):
                user_inputs = parse_user_inputs_from_query(m.content)
                break

    # âœ… ä¿å­˜å› stateï¼Œç¡®ä¿åç»­å¯ä»¥ç”¨
    state["user_inputs"] = user_inputs     
    final_json = None

    # 1ï¸âƒ£ æ‰¾åˆ°åŒ…å« total_cost çš„æœ€ç»ˆ JSON è¾“å‡º
    for m in messages[::-1]:
        try:
            parsed = json.loads(m.content)
            print("[DEBUG] ğŸ§© å°è¯•è§£æ JSONï¼š", parsed)
            if "total_cost" in parsed:
                final_json = parsed
                break
        except Exception as e:
            print(f"[WARN] æ— æ³•è§£ææ¶ˆæ¯: {m.content[:80]}... ({e})")

    if not final_json:
        print("[WARN] æœªæ‰¾åˆ°æœ€ç»ˆ JSON è¾“å‡º")
        return

    # 2ï¸âƒ£ æ·»åŠ åŒ—äº¬æ—¶é—´æ—¶é—´æˆ³ï¼ˆç´§å‡‘æ ¼å¼ï¼‰
    sg_tz = timezone(timedelta(hours=8))
    now = datetime.now(sg_tz)
    timestamp = datetime.now(sg_tz).strftime("%Y%m%d_%H%M%S")
    final_json["timestamp"] = timestamp

    # 3ï¸âƒ£ è·å–åˆé‡‘åï¼ˆé»˜è®¤ UnknownAlloyï¼‰
    alloy_name = final_json.get("alloy_code", "UnknownAlloy")

    # 4ï¸âƒ£ æœ¬åœ°ä¿å­˜è·¯å¾„ï¼ˆæ”¹ä¸ºå®‰å…¨è·¯å¾„é¿å… OneDrive å†²çªï¼‰
    base_dir = os.path.join(os.path.expanduser("~"), "Documents", "costagent_output")
    long_dir = os.path.join(base_dir, "Detailed_Material_Rate")
    short_dir = os.path.join(base_dir, "Brief_Material_Rate")
    os.makedirs(long_dir, exist_ok=True)
    os.makedirs(short_dir, exist_ok=True)

    # 5ï¸âƒ£ æ–‡ä»¶å‘½åï¼šåˆé‡‘å + ç´§å‡‘æ—¶é—´æˆ³
    filename = f"{alloy_name}_{timestamp}.json"
    long_path = os.path.join(long_dir, filename)
    short_path = os.path.join(short_dir, filename)

    # 6ï¸âƒ£ å†™å…¥é•¿çŸ­ JSON æ–‡ä»¶
    with open(long_path, "w", encoding="utf-8") as f:
        json.dump(final_json, f, ensure_ascii=False, indent=2)
        f.flush()
        os.fsync(f.fileno())

    short_json = {
        "timestamp": final_json.get("timestamp"),
        "alloy_code": final_json.get("alloy_code"),
        "unit": final_json.get("unit"),
        "total_cost": final_json.get("total_cost"),
    }
    with open(short_path, "w", encoding="utf-8") as f:
        json.dump(short_json, f, ensure_ascii=False, indent=2)
        f.flush()
        os.fsync(f.fileno())

    print(f"[INFO] âœ… å·²ç”Ÿæˆé•¿ JSON: {long_path}")
    print(f"[INFO] âœ… å·²ç”ŸæˆçŸ­ JSON: {short_path}")

    # ----------------------------------------
    # åˆ é™¤äº† Azure Blob Storage ä¸Šä¼ ç›¸å…³ä»£ç 
    # ----------------------------------------
    
    # ============== âœ… ä¿å­˜åˆ° CSV ==============
    try:
        # æå– state ä¸­çš„å­—æ®µ
        user_inputs = state.get("user_inputs", {})

        # å¿…é¡»å­—æ®µï¼Œæ²¡æœ‰è¾“å…¥çš„ç•™ç©º
        Location = user_inputs.get("Location", "")
        supplier_code = user_inputs.get("supplier_code", "")
        part_number = user_inputs.get("part_number", "")
        sub_process_step = user_inputs.get("sub_process_step", "")
        process_type = user_inputs.get("process_type", "")

        # ä» JSON ä¸­è·å–ææ–™åã€æ—¶é—´ã€æˆæœ¬
        material_name = final_json.get("alloy_code") or final_json.get("material", "Unknown")
        Low = float(final_json.get("total_cost", 0))
        High = round(Low + 2, 2)
        Unit = "/kg"
        valid_time = final_json.get("last_updated", final_json.get("timestamp", ""))
        csv_time = now.strftime("%#m/%#d/%Y")  # ä¾‹å¦‚ 11/3/2025
        source = "web"

        # ç»„ç»‡è¾“å‡ºè¡Œ
        row = {
            "Location": Location,
            "supplier_code": supplier_code,
            "part_number": part_number,
            "sub_process_step": sub_process_step,
            "material_name": material_name,
            "process_type": process_type,
            "Low": Low,
            "High": High,
            "Unit": Unit,
            "valid_time": csv_time,
            "source": source
        }
        # âœ… åˆ›å»º csv æ–‡ä»¶å¤¹
        csv_dir = os.path.join(base_dir, "csv")
        os.makedirs(csv_dir, exist_ok=True)
        csv_path = os.path.join(csv_dir, "cost_records.csv")

        # âœ… å†™å…¥ CSV
        write_header = not os.path.exists(csv_path)
        with open(csv_path, "a", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=row.keys())
            if write_header:
                writer.writeheader()
            writer.writerow(row)

        print(f"âœ… CSV å·²å†™å…¥: {csv_path}")
    except Exception as e:
        print(f"âŒ CSV ä¿å­˜å¤±è´¥: {e}")
    print("ğŸ æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ å®Œæˆã€‚")


import re

def parse_user_inputs_from_query(query: str) -> dict:
    """
    âœ… ä» query ä¸­æå– key=value å¯¹ (æ”¯æŒé€—å·ã€ç©ºæ ¼ã€å¤šå­—æ®µ)
    âœ… è¿”å›å­—å…¸ï¼Œä¾‹å¦‚ï¼š
      {"Location": "Ningbo, Zhejiang", "supplier_code": "97036203", ...}
    """
    result = {}
    pairs = re.findall(r'(\w+)\s*=\s*([^;]+)', query)
    for key, val in pairs:
        result[key.strip()] = val.strip()
    return result

